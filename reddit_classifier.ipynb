{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945cdca3-4a5c-4ce8-b446-6818412f3178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/home/ubuntu/second_study/second_study/lib/python3.8/site-packages/sklearn/base.py:347: InconsistentVersionWarning: Trying to unpickle estimator LinearSVC from version 1.0.1 when using version 1.3.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from reddit_classifier.reddit_models import get_topics\n",
    "from reddit_classifier.reddit_models import get_proba\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f1b678-adde-4d98-aa0a-2ec8c7c669be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "df_male = pd.read_excel(\"results/male.xlsx\").dropna()\n",
    "df_female = pd.read_excel(\"results/female.xlsx\").dropna()\n",
    "\n",
    "data_male = df_male.text.tolist()\n",
    "data_female = df_female.text.tolist() # 26 v. 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a70dc42a-a8a7-4441-a094-95f3c0e28b8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m male_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mget_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_male\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/second_study/reddit_classifier/reddit_models.py:90\u001b[0m, in \u001b[0;36mget_topics\u001b[0;34m(posts)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m posts:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# dictionary for each subreddit's probabilities; need to account for probabilties associated with score=0\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# disregard any score=0 (not looking at what posts are not)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subreddit, model \u001b[38;5;129;01min\u001b[39;00m mh_model_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 90\u001b[0m         score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([\u001b[43mget_liwc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m)\u001b[49m])\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     92\u001b[0m             subreddit_distribution[subreddit] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/second_study/reddit_classifier/reddit_models.py:56\u001b[0m, in \u001b[0;36mget_liwc_embedding\u001b[0;34m(post)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, liwc_category_count):\n\u001b[1;32m     55\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m liwc_lexicon \u001b[38;5;241m=\u001b[39m \u001b[43mLIWCMeta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_liwc_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m post_liwc \u001b[38;5;241m=\u001b[39m LIWCMeta\u001b[38;5;241m.\u001b[39mgetLex(post, liwc_lexicon)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m post_liwc\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/second_study/reddit_classifier/LIWCMeta.py:34\u001b[0m, in \u001b[0;36mextract_liwc_features\u001b[0;34m(mtype)\u001b[0m\n\u001b[1;32m     31\u001b[0m liwc_lexicons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msadness\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_liwc_lexicons(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msadness\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m liwc_lexicons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswear\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_liwc_lexicons(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mswear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m liwc_lexicons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcognitive_mech\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_liwc_lexicons\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcognitive_mech\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m liwc_lexicons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancies\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_liwc_lexicons(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscrepancies\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m liwc_lexicons[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minhibition\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m get_liwc_lexicons(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minhibition\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/second_study/reddit_classifier/LIWCMeta.py:9\u001b[0m, in \u001b[0;36mget_liwc_lexicons\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m lexicon \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m lexicon_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{0}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{1}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliwc_lexicons_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_handle:\n\u001b[1;32m     10\u001b[0m \t\u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file_handle:\n\u001b[1;32m     11\u001b[0m \t\tlexicon_item \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/usr/lib/python3.8/_bootlocale.py:33\u001b[0m, in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m locale\u001b[38;5;241m.\u001b[39mgetpreferredencoding(do_setlocale)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetpreferredencoding\u001b[39m(do_setlocale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m do_setlocale\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mutf8_mode:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "male_distribution = get_topics(data_male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07938440-9e36-4554-ad17-80682c12376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psychosis': 168, 'stress': 168, 'selfharm': 168, 'schizophrenia': 168, 'SuicideWatch': 168, 'depression': 168, 'anxiety': 168}\n"
     ]
    }
   ],
   "source": [
    "print(male_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53f2dbef-ff35-46bd-8b37-4d7f5f999647",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_distribution = get_topics(data_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0601a981-e9ab-4a60-ace0-db2df478a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psychosis': 163, 'stress': 164, 'selfharm': 164, 'schizophrenia': 164, 'SuicideWatch': 163, 'depression': 163, 'anxiety': 163}\n"
     ]
    }
   ],
   "source": [
    "print(female_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee3aa6e-959e-45a9-bea4-42cb50a3cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'psychosis': 0, 'stress': 0, 'selfharm': 27, 'schizophrenia': 0, 'SuicideWatch': 77, 'depression': 131, 'anxiety': 110}\n"
     ]
    }
   ],
   "source": [
    "male_proba = get_proba(data_male)\n",
    "print(male_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfdb08-2b05-44da-bb2f-c59bc6edd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_proba = get_proba(data_female)\n",
    "print(female_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddca13fc-cb62-4045-ad4b-92111131abb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "168\n"
     ]
    }
   ],
   "source": [
    "print(len(data_female))\n",
    "print(len(data_male))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc53be9-e07a-4783-b43d-df726a4bb3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "second_study",
   "language": "python",
   "name": "second_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
